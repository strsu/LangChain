import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.chains import RetrievalQA
from langchain.llms import Ollama
import tempfile
import os
import hashlib
import json
from datetime import datetime
from typing import List, Dict
import shutil

# ìƒìˆ˜ ì •ì˜
UPLOAD_DIR = "uploaded_pdfs"
PDF_INFO_FILE = "pdf_info.json"
CHROMA_DIR = "chroma_dbs"

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(CHROMA_DIR, exist_ok=True)

# PDF ì •ë³´ ë¡œë“œ
def load_pdf_info():
    if os.path.exists(PDF_INFO_FILE):
        with open(PDF_INFO_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

# PDF ì •ë³´ ì €ì¥
def save_pdf_info(pdf_info):
    with open(PDF_INFO_FILE, 'w', encoding='utf-8') as f:
        json.dump(pdf_info, f, ensure_ascii=False, indent=2)

# PDF íŒŒì¼ì˜ í•´ì‹œê°’ ìƒì„±
def get_file_hash(file_content):
    return hashlib.md5(file_content).hexdigest()

# ì„ íƒëœ PDFë“¤ì˜ ë²¡í„° ìŠ¤í† ì–´ ê²°í•©
def combine_vectorstores(pdf_hashes: List[str], pdf_info: Dict, embedding_model) -> Chroma:
    if len(pdf_hashes) == 1:
        return Chroma(
            persist_directory=pdf_info[pdf_hashes[0]]["chroma_dir"],
            embedding_function=embedding_model
        )
    
    # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ê²°í•©ëœ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    temp_dir = os.path.join(CHROMA_DIR, "temp_combined")
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)
    
    # ì²« ë²ˆì§¸ ë²¡í„° ìŠ¤í† ì–´ ë³µì‚¬
    shutil.copytree(pdf_info[pdf_hashes[0]]["chroma_dir"], temp_dir)
    combined_db = Chroma(
        persist_directory=temp_dir,
        embedding_function=embedding_model
    )
    
    # ë‚˜ë¨¸ì§€ ë²¡í„° ìŠ¤í† ì–´ì˜ ë°ì´í„° ì¶”ê°€
    for pdf_hash in pdf_hashes[1:]:
        db = Chroma(
            persist_directory=pdf_info[pdf_hash]["chroma_dir"],
            embedding_function=embedding_model
        )
        combined_db._collection.add(
            embeddings=db._collection.get()["embeddings"],
            documents=db._collection.get()["documents"],
            metadatas=db._collection.get()["metadatas"],
            ids=db._collection.get()["ids"]
        )
    
    return combined_db

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="PDF QA ì±—ë´‡", layout="wide")
st.title("ğŸ“„ PDF ì—…ë¡œë“œ + ì§ˆë¬¸ ë‹µë³€ ì±—ë´‡ (í•œê¸€ ì§€ì›)")

# PDF ì •ë³´ ë¡œë“œ
pdf_info = load_pdf_info()

# ì‚¬ì´ë“œë°”ì— PDF ëª©ë¡ í‘œì‹œ
st.sidebar.title("ğŸ“š ì €ì¥ëœ PDF ëª©ë¡")

# PDF ê·¸ë£¹í™” (íŒŒì¼ëª… ê¸°ì¤€)
pdf_groups = {}
for pdf_hash, info in pdf_info.items():
    base_name = info["filename"].rsplit(".", 1)[0]  # í™•ì¥ì ì œì™¸
    if base_name not in pdf_groups:
        pdf_groups[base_name] = []
    pdf_groups[base_name].append(pdf_hash)

# ìƒˆ PDF ì—…ë¡œë“œ ì²˜ë¦¬
uploaded_file = st.sidebar.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])
if uploaded_file:
    file_content = uploaded_file.read()
    file_hash = get_file_hash(file_content)
    
    if file_hash in pdf_info:
        st.sidebar.warning(f"'{uploaded_file.name}'ëŠ” ì´ë¯¸ ì—…ë¡œë“œëœ íŒŒì¼ì…ë‹ˆë‹¤!")
    else:
        pdf_path = os.path.join(UPLOAD_DIR, f"{file_hash}.pdf")
        with open(pdf_path, 'wb') as f:
            f.write(file_content)
        
        pdf_info[file_hash] = {
            "filename": uploaded_file.name,
            "upload_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "path": pdf_path,
            "chroma_dir": os.path.join(CHROMA_DIR, file_hash)
        }
        save_pdf_info(pdf_info)
        
        with st.spinner("PDF ì²˜ë¦¬ ì¤‘..."):
            loader = PyPDFLoader(pdf_path)
            documents = loader.load()
            
            splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
            docs = splitter.split_documents(documents)
            
            embedding_model = SentenceTransformerEmbeddings(
                model_name="sentence-transformers/clip-ViT-B-32-multilingual-v1"
            )
            
            Chroma.from_documents(
                documents=docs,
                embedding=embedding_model,
                persist_directory=pdf_info[file_hash]["chroma_dir"]
            )
        
        st.sidebar.success(f"'{uploaded_file.name}' ì—…ë¡œë“œ ë° ì²˜ë¦¬ ì™„ë£Œ!")
        st.rerun()

# PDF ì„ íƒ UI
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ“‘ ë¶„ì„í•  PDF ì„ íƒ")

selected_pdfs = []
for group_name, pdf_hashes in pdf_groups.items():
    st.sidebar.markdown(f"**{group_name}**")
    for pdf_hash in pdf_hashes:
        info = pdf_info[pdf_hash]
        if st.sidebar.checkbox(
            f"{info['filename']} ({info['upload_date']})",
            key=f"select_{pdf_hash}"
        ):
            selected_pdfs.append(pdf_hash)

# ì„ íƒëœ PDFê°€ ìˆì„ ë•Œë§Œ ì²˜ë¦¬
if selected_pdfs:
    # ì„ íƒëœ PDF ì •ë³´ í‘œì‹œ
    st.markdown("### ğŸ“Œ ì„ íƒëœ PDF ëª©ë¡:")
    for pdf_hash in selected_pdfs:
        st.info(f"- {pdf_info[pdf_hash]['filename']} (ì—…ë¡œë“œ: {pdf_info[pdf_hash]['upload_date']})")
    
    # ë²¡í„° ìŠ¤í† ì–´ ê²°í•©
    embedding_model = SentenceTransformerEmbeddings(
        model_name="sentence-transformers/clip-ViT-B-32-multilingual-v1"
    )
    
    vectordb = combine_vectorstores(selected_pdfs, pdf_info, embedding_model)
    
    # LLM ì´ˆê¸°í™”
    llm = Ollama(model="mistral")
    
    # Retrieval QA ì²´ì¸ ìƒì„±
    qa = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectordb.as_retriever(),
        return_source_documents=True
    )
    
    # ì§ˆë¬¸ ì…ë ¥ UI
    query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ì´ ë¬¸ì„œë“¤ì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?")
    
    if query:
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            result = qa({"query": query})
        
        st.markdown("### ğŸ’¬ ë‹µë³€:")
        st.write(result["result"])
        
        # ì¶œì²˜ ë¬¸ì„œ ë³´ê¸°
        with st.expander("ğŸ” ë‹µë³€ ê·¼ê±° ë¬¸ì„œ ë³´ê¸°"):
            for i, doc in enumerate(result["source_documents"]):
                source_pdf = doc.metadata.get("source", "").split("/")[-1].split(".")[0]
                st.markdown(f"**ë¬¸ì„œ chunk {i+1} (ì¶œì²˜: {pdf_info.get(source_pdf, {}).get('filename', 'ì•Œ ìˆ˜ ì—†ìŒ')}):**")
                st.write(doc.page_content)
    
    # PDF ì‚­ì œ ê¸°ëŠ¥
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ—‘ PDF ì‚­ì œ")
    for pdf_hash in selected_pdfs:
        if st.sidebar.button(f"ì‚­ì œ: {pdf_info[pdf_hash]['filename']}", key=f"delete_{pdf_hash}"):
            os.remove(pdf_info[pdf_hash]["path"])
            shutil.rmtree(pdf_info[pdf_hash]["chroma_dir"], ignore_errors=True)
            del pdf_info[pdf_hash]
            save_pdf_info(pdf_info)
            st.sidebar.success("PDFê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.rerun()

else:
    st.info("ë¶„ì„í•  PDFë¥¼ ì„ íƒí•˜ê±°ë‚˜ ìƒˆë¡œìš´ PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

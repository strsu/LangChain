import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA, LLMChain
from langchain.prompts import PromptTemplate
from langchain_community.llms import Ollama
import tempfile
import os
import hashlib
import json
from datetime import datetime
from typing import List, Dict
import shutil
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from keybert import KeyBERT
import pandas as pd
import nltk
from collections import defaultdict
import requests
import subprocess
import time

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# ìƒìˆ˜ ì •ì˜
UPLOAD_DIR = "uploaded_pdfs"
PDF_INFO_FILE = "pdf_info.json"
CHROMA_DIR = "chroma_dbs"
CHAT_HISTORY_FILE = "chat_history.json"
GENERAL_CHAT_KEY = "general_chat"  # ì¼ë°˜ ëŒ€í™”ìš© í‚¤

# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡
AVAILABLE_MODELS = {
    "tinyllama": "ê°€ë²¼ìš´ ëª¨ë¸ (512MB)",
    "llama2": "ì¤‘ê°„ í¬ê¸° ëª¨ë¸ (3GB)",
    "mistral": "í° ëª¨ë¸ (4GB)",
    "neural-chat": "ì‘ì€ ëŒ€í™” íŠ¹í™” ëª¨ë¸ (1.5GB)"
}

# Ollama ëª¨ë¸ ì„¤ì¹˜ í™•ì¸ ë° ì„¤ì¹˜
def check_and_install_model(model_name: str) -> bool:
    # Ollama ì„œë¹„ìŠ¤ í™•ì¸
    try:
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code != 200:
            return False
    except requests.exceptions.ConnectionError:
        st.error("""
        Ollama ì„œë¹„ìŠ¤ê°€ ì‹¤í–‰ë˜ê³  ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¼ì£¼ì„¸ìš”:
        
        1. Ollama ì„¤ì¹˜ (ì²˜ìŒ ì‹¤í–‰ ì‹œ):
        ```bash
        curl -fsSL https://ollama.com/install.sh | sh
        ```
        
        2. Ollama ì„œë¹„ìŠ¤ ì‹¤í–‰:
        ```bash
        ollama serve
        ```
        """)
        return False
    
    # ì„¤ì¹˜ëœ ëª¨ë¸ í™•ì¸
    installed_models = [tag["name"] for tag in response.json().get("models", [])]
    if model_name not in installed_models:
        with st.spinner(f"'{model_name}' ëª¨ë¸ ì„¤ì¹˜ ì¤‘... (ì²˜ìŒ ì‹¤í–‰ì‹œ ëª‡ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
            try:
                subprocess.run(["ollama", "pull", model_name], check=True)
                time.sleep(2)  # ì„¤ì¹˜ ì™„ë£Œ í›„ ì ì‹œ ëŒ€ê¸°
                return True
            except subprocess.CalledProcessError:
                st.error(f"""
                '{model_name}' ëª¨ë¸ ì„¤ì¹˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.
                í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”:
                ```bash
                ollama pull {model_name}
                ```
                """)
                return False
    return True

def is_model_installed(model_name: str) -> bool:
    try:
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            installed_models = [tag["name"] for tag in response.json().get("models", [])]
            return model_name in installed_models
    except requests.exceptions.ConnectionError:
        return False
    return False

# ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(CHROMA_DIR, exist_ok=True)

# ì±„íŒ… íˆìŠ¤í† ë¦¬ ê´€ë¦¬ í•¨ìˆ˜
def load_chat_history():
    if os.path.exists(CHAT_HISTORY_FILE):
        with open(CHAT_HISTORY_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return defaultdict(list)

def save_chat_history(history):
    with open(CHAT_HISTORY_FILE, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = load_chat_history()

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="AI ì±—ë´‡", layout="wide")

# ì‚¬ì´ë“œë°”ì— ëª¨ë¸ ì„ íƒ ì¶”ê°€
st.sidebar.title("ğŸ¤– ëª¨ë¸ ì„¤ì •")
selected_model = st.sidebar.selectbox(
    "ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ",
    options=list(AVAILABLE_MODELS.keys()),
    format_func=lambda x: f"{x} - {AVAILABLE_MODELS[x]}",
    index=0  # ê¸°ë³¸ê°’ìœ¼ë¡œ tinyllama ì„ íƒ
)

# ëª¨ë¸ ì„¤ì¹˜ í™•ì¸
if not check_and_install_model(selected_model):
    st.stop()

# LLM ì´ˆê¸°í™”
@st.cache_resource
def get_llm(model_name):
    return Ollama(model=model_name)

llm = get_llm(selected_model)

# ëª¨ë¸ ìƒíƒœì— ë”°ë¥¸ ì‚¬ì´ë“œë°” ì •ë³´ í‘œì‹œ
sidebar_info = f"""
í˜„ì¬ í™˜ê²½: CPU 2ì½”ì–´, RAM 16GB
ì„ íƒëœ ëª¨ë¸: {selected_model}
ëª¨ë¸ ì„¤ëª…: {AVAILABLE_MODELS[selected_model]}
"""

if not is_model_installed(selected_model):
    sidebar_info += f"""
ğŸ’¡ ëª¨ë¸ ê´€ë¦¬ ëª…ë ¹ì–´:
```bash
# ëª¨ë¸ ì„¤ì¹˜
ollama pull {selected_model}

# ì„¤ì¹˜ëœ ëª¨ë¸ ëª©ë¡ í™•ì¸
ollama list

# ëª¨ë¸ ì œê±°
ollama rm {selected_model}
```
"""

st.sidebar.info(sidebar_info)

# ì¼ë°˜ ëŒ€í™”ìš© í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
general_chat_prompt = PromptTemplate(
    template="""ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ëŒ€í™”í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì‚¬í•­ì„ ì§€ì¼œì£¼ì„¸ìš”:
1. í•­ìƒ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ê¸°
2. ë²ˆì—­í•˜ì§€ ì•Šê³  ë°”ë¡œ í•œêµ­ì–´ë¡œ ìƒê°í•˜ê³  ë‹µë³€í•˜ê¸°
3. ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•˜ê¸°
4. í•„ìš”í•œ ê²½ìš° ì˜ˆì‹œë‚˜ êµ¬ì²´ì ì¸ ì„¤ëª… ì¶”ê°€í•˜ê¸°

ì‚¬ìš©ìì˜ ì§ˆë¬¸: {question}

ë‹µë³€:""",
    input_variables=["question"]
)

general_chat_chain = LLMChain(
    llm=llm,
    prompt=general_chat_prompt
)

# ë¬¸ì„œ ë¶„ì„ í•¨ìˆ˜ë“¤
def extract_keywords(text: str, top_n: int = 10) -> List[tuple]:
    kw_model = KeyBERT()
    keywords = kw_model.extract_keywords(text, 
                                       keyphrase_ngram_range=(1, 2),
                                       stop_words='english', 
                                       top_n=top_n)
    return keywords

def generate_wordcloud(text: str):
    wordcloud = WordCloud(width=800, height=400,
                         background_color='white',
                         font_path='/usr/share/fonts/truetype/nanum/NanumGothic.ttf'
                         ).generate(text)
    
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    return plt

def calculate_document_similarity(docs1, docs2, embedding_model):
    # ë¬¸ì„œ ì„ë² ë”© ê³„ì‚°
    embeddings1 = embedding_model.embed_documents([doc.page_content for doc in docs1])
    embeddings2 = embedding_model.embed_documents([doc.page_content for doc in docs2])
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarity = cosine_similarity(embeddings1, embeddings2)
    return np.mean(similarity)

# PDF ì •ë³´ ë¡œë“œ
def load_pdf_info():
    if os.path.exists(PDF_INFO_FILE):
        with open(PDF_INFO_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}

# PDF ì •ë³´ ì €ì¥
def save_pdf_info(pdf_info):
    with open(PDF_INFO_FILE, 'w', encoding='utf-8') as f:
        json.dump(pdf_info, f, ensure_ascii=False, indent=2)

# PDF íŒŒì¼ì˜ í•´ì‹œê°’ ìƒì„±
def get_file_hash(file_content):
    return hashlib.md5(file_content).hexdigest()

# ì„ íƒëœ PDFë“¤ì˜ ë²¡í„° ìŠ¤í† ì–´ ê²°í•©
def combine_vectorstores(pdf_hashes: List[str], pdf_info: Dict, embedding_model) -> Chroma:
    if len(pdf_hashes) == 1:
        return Chroma(
            persist_directory=pdf_info[pdf_hashes[0]]["chroma_dir"],
            embedding_function=embedding_model
        )
    
    # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ê²°í•©ëœ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    temp_dir = os.path.join(CHROMA_DIR, "temp_combined")
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)
    
    # ì²« ë²ˆì§¸ ë²¡í„° ìŠ¤í† ì–´ ë³µì‚¬
    shutil.copytree(pdf_info[pdf_hashes[0]]["chroma_dir"], temp_dir)
    combined_db = Chroma(
        persist_directory=temp_dir,
        embedding_function=embedding_model
    )
    
    # ë‚˜ë¨¸ì§€ ë²¡í„° ìŠ¤í† ì–´ì˜ ë°ì´í„° ì¶”ê°€
    for pdf_hash in pdf_hashes[1:]:
        db = Chroma(
            persist_directory=pdf_info[pdf_hash]["chroma_dir"],
            embedding_function=embedding_model
        )
        combined_db._collection.add(
            embeddings=db._collection.get()["embeddings"],
            documents=db._collection.get()["documents"],
            metadatas=db._collection.get()["metadatas"],
            ids=db._collection.get()["ids"]
        )
    
    return combined_db

# íƒ­ ìƒì„±
tab1, tab2, tab3, tab4 = st.tabs(["ğŸ’¬ ì¼ë°˜ ëŒ€í™”", "ğŸ“„ PDF ë¶„ì„", "ğŸ“Š ë¬¸ì„œ ë¶„ì„", "ğŸ“ ëŒ€í™” ê¸°ë¡"])

# ì¼ë°˜ ëŒ€í™” íƒ­
with tab1:
    st.title("ğŸ’¬ AI ì±—ë´‡ê³¼ ëŒ€í™”í•˜ê¸°")
    
    chat_input = st.text_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”", key="general_chat_input")
    
    if chat_input:
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            response = general_chat_chain.invoke({"question": chat_input})
            
            st.markdown("### ğŸ’¬ ë‹µë³€:")
            st.write(response['text'])  # invokeëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ 'text' í‚¤ë¡œ ì ‘ê·¼
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥
            st.session_state.chat_history[GENERAL_CHAT_KEY].append({
                'question': chat_input,
                'answer': response['text'],
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            })
            save_chat_history(st.session_state.chat_history)

# PDF ë¶„ì„ íƒ­
with tab2:
    st.title("ğŸ“„ PDF ì—…ë¡œë“œ + ì§ˆë¬¸ ë‹µë³€")
    
    # PDF ì •ë³´ ë¡œë“œ
    pdf_info = load_pdf_info()

    # ì‚¬ì´ë“œë°”ì— PDF ëª©ë¡ í‘œì‹œ
    st.sidebar.title("ğŸ“š ì €ì¥ëœ PDF ëª©ë¡")

    # PDF ê·¸ë£¹í™” (íŒŒì¼ëª… ê¸°ì¤€)
    pdf_groups = {}
    for pdf_hash, info in pdf_info.items():
        base_name = info["filename"].rsplit(".", 1)[0]  # í™•ì¥ì ì œì™¸
        if base_name not in pdf_groups:
            pdf_groups[base_name] = []
        pdf_groups[base_name].append(pdf_hash)

    # ìƒˆ PDF ì—…ë¡œë“œ ì²˜ë¦¬
    uploaded_file = st.sidebar.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type=["pdf"])
    if uploaded_file:
        file_content = uploaded_file.read()
        file_hash = get_file_hash(file_content)
        
        if file_hash in pdf_info:
            st.sidebar.warning(f"'{uploaded_file.name}'ëŠ” ì´ë¯¸ ì—…ë¡œë“œëœ íŒŒì¼ì…ë‹ˆë‹¤!")
        else:
            pdf_path = os.path.join(UPLOAD_DIR, f"{file_hash}.pdf")
            with open(pdf_path, 'wb') as f:
                f.write(file_content)
            
            pdf_info[file_hash] = {
                "filename": uploaded_file.name,
                "upload_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "path": pdf_path,
                "chroma_dir": os.path.join(CHROMA_DIR, file_hash)
            }
            save_pdf_info(pdf_info)
            
            with st.spinner("PDF ì²˜ë¦¬ ì¤‘..."):
                loader = PyPDFLoader(pdf_path)
                documents = loader.load()
                
                splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
                docs = splitter.split_documents(documents)
                
                embedding_model = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/clip-ViT-B-32-multilingual-v1"
                )
                
                Chroma.from_documents(
                    documents=docs,
                    embedding=embedding_model,
                    persist_directory=pdf_info[file_hash]["chroma_dir"]
                )
            
            st.sidebar.success(f"'{uploaded_file.name}' ì—…ë¡œë“œ ë° ì²˜ë¦¬ ì™„ë£Œ!")
            st.rerun()

    # PDF ì„ íƒ UI
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ“‘ ë¶„ì„í•  PDF ì„ íƒ")

    selected_pdfs = []
    for group_name, pdf_hashes in pdf_groups.items():
        st.sidebar.markdown(f"**{group_name}**")
        for pdf_hash in pdf_hashes:
            info = pdf_info[pdf_hash]
            if st.sidebar.checkbox(
                f"{info['filename']} ({info['upload_date']})",
                key=f"select_{pdf_hash}"
            ):
                selected_pdfs.append(pdf_hash)

    # ì„ íƒëœ PDFê°€ ìˆì„ ë•Œë§Œ ì²˜ë¦¬
    if selected_pdfs:
        # ì„ íƒëœ PDF ì •ë³´ í‘œì‹œ
        st.markdown("### ğŸ“Œ ì„ íƒëœ PDF ëª©ë¡:")
        for pdf_hash in selected_pdfs:
            st.info(f"- {pdf_info[pdf_hash]['filename']} (ì—…ë¡œë“œ: {pdf_info[pdf_hash]['upload_date']})")
        
        # ë²¡í„° ìŠ¤í† ì–´ ê²°í•©
        embedding_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/clip-ViT-B-32-multilingual-v1"
        )
        
        vectordb = combine_vectorstores(selected_pdfs, pdf_info, embedding_model)
        
        # Retrieval QA ì²´ì¸ ìƒì„±
        qa = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=vectordb.as_retriever(),
            return_source_documents=True
        )
        
        # ì§ˆë¬¸ ì…ë ¥ UI
        query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ì´ ë¬¸ì„œë“¤ì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?")
        
        if query:
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                result = qa.invoke({"query": query})
            
            st.markdown("### ğŸ’¬ ë‹µë³€:")
            st.write(result["result"])
            
            # ì¶œì²˜ ë¬¸ì„œ ë³´ê¸°
            with st.expander("ğŸ” ë‹µë³€ ê·¼ê±° ë¬¸ì„œ ë³´ê¸°"):
                for i, doc in enumerate(result["source_documents"]):
                    source_pdf = doc.metadata.get("source", "").split("/")[-1].split(".")[0]
                    st.markdown(f"**ë¬¸ì„œ chunk {i+1} (ì¶œì²˜: {pdf_info.get(source_pdf, {}).get('filename', 'ì•Œ ìˆ˜ ì—†ìŒ')}):**")
                    st.write(doc.page_content)
            
            # ì±„íŒ… íˆìŠ¤í† ë¦¬ì— ëŒ€í™” ì¶”ê°€
            chat_key = ','.join(sorted(selected_pdfs))
            st.session_state.chat_history[chat_key].append({
                'question': query,
                'answer': result["result"],
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            })
            save_chat_history(st.session_state.chat_history)

    # PDF ì‚­ì œ ê¸°ëŠ¥
    st.sidebar.markdown("---")
    st.sidebar.subheader("ğŸ—‘ PDF ì‚­ì œ")
    for pdf_hash in selected_pdfs:
        if st.sidebar.button(f"ì‚­ì œ: {pdf_info[pdf_hash]['filename']}", key=f"delete_{pdf_hash}"):
            os.remove(pdf_info[pdf_hash]["path"])
            shutil.rmtree(pdf_info[pdf_hash]["chroma_dir"], ignore_errors=True)
            del pdf_info[pdf_hash]
            save_pdf_info(pdf_info)
            st.sidebar.success("PDFê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            st.rerun()

    else:
        st.info("ë¶„ì„í•  PDFë¥¼ ì„ íƒí•˜ê±°ë‚˜ ìƒˆë¡œìš´ PDFë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

# ë¬¸ì„œ ë¶„ì„ íƒ­
with tab3:
    st.title("ğŸ“Š ë¬¸ì„œ ë¶„ì„")
    
    if selected_pdfs:
        # ë¶„ì„ ì˜µì…˜ ì„ íƒ
        analysis_type = st.selectbox(
            "ë¶„ì„ ìœ í˜• ì„ íƒ",
            ["ë¬¸ì„œ ìš”ì•½", "í‚¤ì›Œë“œ ë¶„ì„", "ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„", "ì›Œë“œí´ë¼ìš°ë“œ"]
        )
        
        if analysis_type == "ë¬¸ì„œ ìš”ì•½":
            with st.spinner("ë¬¸ì„œ ìš”ì•½ ì¤‘..."):
                summary_prompt = PromptTemplate(
                    template="ë‹¤ìŒ ë¬¸ì„œë¥¼ 500ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{text}\n\nìš”ì•½:",
                    input_variables=["text"]
                )
                
                summary_chain = LLMChain(
                    llm=llm,
                    prompt=summary_prompt
                )
                
                for pdf_hash in selected_pdfs:
                    st.subheader(f"ğŸ“‘ {pdf_info[pdf_hash]['filename']}")
                    loader = PyPDFLoader(pdf_info[pdf_hash]['path'])
                    documents = loader.load()
                    text = "\n".join([doc.page_content for doc in documents])
                    summary_result = summary_chain.invoke({"text": text})
                    st.write(summary_result['text'])
        
        elif analysis_type == "í‚¤ì›Œë“œ ë¶„ì„":
            with st.spinner("í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘..."):
                for pdf_hash in selected_pdfs:
                    st.subheader(f"ğŸ“‘ {pdf_info[pdf_hash]['filename']}")
                    loader = PyPDFLoader(pdf_info[pdf_hash]['path'])
                    documents = loader.load()
                    text = "\n".join([doc.page_content for doc in documents])
                    keywords = extract_keywords(text)
                    
                    # í‚¤ì›Œë“œ ì‹œê°í™”
                    df = pd.DataFrame(keywords, columns=['Keyword', 'Score'])
                    st.bar_chart(df.set_index('Keyword')['Score'])
        
        elif analysis_type == "ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„":
            if len(selected_pdfs) < 2:
                st.warning("ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” 2ê°œ ì´ìƒì˜ PDFë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            else:
                with st.spinner("ë¬¸ì„œ ìœ ì‚¬ë„ ë¶„ì„ ì¤‘..."):
                    similarity_matrix = []
                    for i, pdf1 in enumerate(selected_pdfs):
                        row = []
                        for j, pdf2 in enumerate(selected_pdfs):
                            if i == j:
                                row.append(1.0)
                            elif j > i:
                                loader1 = PyPDFLoader(pdf_info[pdf1]['path'])
                                loader2 = PyPDFLoader(pdf_info[pdf2]['path'])
                                docs1 = loader1.load()
                                docs2 = loader2.load()
                                similarity = calculate_document_similarity(docs1, docs2, embedding_model)
                                row.append(similarity)
                            else:
                                row.append(similarity_matrix[j][i])
                        similarity_matrix.append(row)
                    
                    # ìœ ì‚¬ë„ í–‰ë ¬ ì‹œê°í™”
                    fig, ax = plt.subplots(figsize=(10, 8))
                    im = ax.imshow(similarity_matrix, cmap='YlOrRd')
                    
                    # ì¶• ë ˆì´ë¸” ì„¤ì •
                    filenames = [pdf_info[pdf]['filename'] for pdf in selected_pdfs]
                    ax.set_xticks(np.arange(len(filenames)))
                    ax.set_yticks(np.arange(len(filenames)))
                    ax.set_xticklabels(filenames, rotation=45, ha='right')
                    ax.set_yticklabels(filenames)
                    
                    # ì»¬ëŸ¬ë°” ì¶”ê°€
                    plt.colorbar(im)
                    plt.tight_layout()
                    st.pyplot(fig)
        
        elif analysis_type == "ì›Œë“œí´ë¼ìš°ë“œ":
            with st.spinner("ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘..."):
                for pdf_hash in selected_pdfs:
                    st.subheader(f"ğŸ“‘ {pdf_info[pdf_hash]['filename']}")
                    loader = PyPDFLoader(pdf_info[pdf_hash]['path'])
                    documents = loader.load()
                    text = "\n".join([doc.page_content for doc in documents])
                    fig = generate_wordcloud(text)
                    st.pyplot(fig)

# ëŒ€í™” ê¸°ë¡ íƒ­
with tab4:
    st.title("ğŸ“ ëŒ€í™” ê¸°ë¡")
    
    # ëŒ€í™” ìœ í˜• ì„ íƒ
    chat_type = st.radio(
        "ëŒ€í™” ìœ í˜• ì„ íƒ",
        ["ì¼ë°˜ ëŒ€í™”", "PDF ë¶„ì„ ëŒ€í™”"],
        horizontal=True
    )
    
    if chat_type == "ì¼ë°˜ ëŒ€í™”":
        if GENERAL_CHAT_KEY in st.session_state.chat_history and st.session_state.chat_history[GENERAL_CHAT_KEY]:
            for chat in st.session_state.chat_history[GENERAL_CHAT_KEY]:
                with st.expander(f"ğŸ•’ {chat['timestamp']} - {chat['question'][:50]}..."):
                    st.markdown("**ì§ˆë¬¸:**")
                    st.write(chat['question'])
                    st.markdown("**ë‹µë³€:**")
                    st.write(chat['answer'])
                    
                    # ë‹µë³€ í‰ê°€ ë²„íŠ¼
                    col1, col2 = st.columns(2)
                    with col1:
                        if st.button("ğŸ‘ ë„ì›€ë¨", key=f"helpful_general_{chat['timestamp']}"):
                            st.success("í”¼ë“œë°± ê°ì‚¬í•©ë‹ˆë‹¤!")
                    with col2:
                        if st.button("ğŸ‘ ë„ì›€ì•ˆë¨", key=f"not_helpful_general_{chat['timestamp']}"):
                            st.error("í”¼ë“œë°± ê°ì‚¬í•©ë‹ˆë‹¤!")
        else:
            st.info("ì•„ì§ ì¼ë°˜ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    else:  # PDF ë¶„ì„ ëŒ€í™”
        if st.session_state.chat_history:
            for pdf_group, history in st.session_state.chat_history.items():
                if pdf_group == GENERAL_CHAT_KEY:  # ì¼ë°˜ ëŒ€í™”ëŠ” ê±´ë„ˆë›°ê¸°
                    continue
                    
                if not history:  # ë¹ˆ íˆìŠ¤í† ë¦¬ëŠ” ê±´ë„ˆë›°ê¸°
                    continue
                
                pdf_names = [
                    pdf_info[pdf_hash]['filename'] 
                    for pdf_hash in pdf_group.split(',') 
                    if pdf_hash in pdf_info
                ]
                if not pdf_names:  # PDFê°€ ì‚­ì œëœ ê²½ìš° ê±´ë„ˆë›°ê¸°
                    continue
                    
                st.subheader(f"ğŸ“š ë¬¸ì„œ: {', '.join(pdf_names)}")
                
                for chat in history:
                    with st.expander(f"ğŸ•’ {chat['timestamp']} - {chat['question'][:50]}..."):
                        st.markdown("**ì§ˆë¬¸:**")
                        st.write(chat['question'])
                        st.markdown("**ë‹µë³€:**")
                        st.write(chat['answer'])
                        
                        # ë‹µë³€ í‰ê°€ ë²„íŠ¼
                        col1, col2 = st.columns(2)
                        with col1:
                            if st.button("ğŸ‘ ë„ì›€ë¨", key=f"helpful_{chat['timestamp']}"):
                                st.success("í”¼ë“œë°± ê°ì‚¬í•©ë‹ˆë‹¤!")
                        with col2:
                            if st.button("ğŸ‘ ë„ì›€ì•ˆë¨", key=f"not_helpful_{chat['timestamp']}"):
                                st.error("í”¼ë“œë°± ê°ì‚¬í•©ë‹ˆë‹¤!")
        else:
            st.info("ì•„ì§ PDF ë¶„ì„ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
